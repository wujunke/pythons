#coding=utf-8
import json
import os
import re
import threading
import urllib2

import requests
from bs4 import BeautifulSoup
import time
from random import choice, random

from cookielib import LWPCookieJar

f = open('URLtext2','r')
urllist = f.readlines()
iplist=['124.115.157.54','114.215.102.168:8081','111.56.5.42','39.91.107.250:8088','180.153.58.154:8088','58.210.218.106:80']


# s = requests.session()
heders = {
            'Accept':'application/json, text/javascript, */*; q=0.01',
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'zh-CN,zh;q=0.8,en;q=0.6',
            'Connection': 'keep-alive',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Host':'radar.itjuzi.com',
            'Referer':'http://radar.itjuzi.com///company?phpSessId=e65ca8471446469d5e68b8885ff06f67fc0d31db?phpSessId=d87230bfa03a3885aa4471da7ab09491948fff74',
            'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
            'X-Requested-With':'XMLHttpRequest',
            # 'Cookie':'gr_user_id=7426f888-1354-4f7f-80c0-531fec0b0d03; pgv_pvi=7091623936; acw_tc=AQAAAOI4Zl/NTAUAolf5NrvXJHfZa0ak; pgv_si=s5223340032; identity=18616837957%40test.com; remember_code=g6oxsJobm9; unique_token=439977; Hm_lvt_1c587ad486cdb6b962e94fc2002edf89=1507774585,1507786866,1507864598,1507882898; Hm_lpvt_1c587ad486cdb6b962e94fc2002edf89=1508135327; gr_session_id_eee5a46c52000d401f969f4535bdaa78=269945b4-b1b2-4b24-9d8c-a1f6e98db99f; gr_cs1_269945b4-b1b2-4b24-9d8c-a1f6e98db99f=user_id%3A439977; Hm_lvt_80ec13defd46fe15d2c2dcf90450d14b=1507774907,1507787708,1507864595; Hm_lpvt_80ec13defd46fe15d2c2dcf90450d14b=1508141587; _ga=GA1.2.516769438.1507774585; _gid=GA1.2.854034579.1508121282; _gat=1; _hp2_ses_props.2147584538=%7B%22r%22%3A%22http%3A%2F%2Fradar.itjuzi.com%2Finvestevent%2Fmergout%22%2C%22ts%22%3A1508141575190%2C%22d%22%3A%22radar.itjuzi.com%22%2C%22h%22%3A%22%2Finvestevent%22%7D; _hp2_id.2147584538=%7B%22userId%22%3A%227435351535902841%22%2C%22pageviewId%22%3A%221377276180301949%22%2C%22sessionId%22%3A%220642825049713276%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%223.0%22%7D; session=831ab884be37c4e3941148251183818f80535fa0; user-radar.itjuzi.com=%7B%22n%22%3A%22%5Cu6854%5Cu53cb9ef285e61d740%22%2C%22v%22%3A0%7D',
            'Cookie':'gr_user_id=73971208-0450-4872-aa64-b6d28b047857; acw_tc=AQAAAGtfIgvwXwUAolf5NpG2Q+C0LzLZ; identity=xzm920%40163.com; remember_code=0HyMcpVQ01; unique_token=329559; pgv_pvi=571540480; pgv_si=s6454591488; Hm_lvt_1c587ad486cdb6b962e94fc2002edf89=1507775466,1507775501,1507775505; Hm_lpvt_1c587ad486cdb6b962e94fc2002edf89=1508141957; session=4233b5a048bccc78d5138a000c1ead1d7cbca46c; user-radar.itjuzi.com=%7B%22n%22%3A%22xzm920%22%2C%22v%22%3A2%7D; _hp2_ses_props.2147584538=%7B%22ts%22%3A1508141966096%2C%22d%22%3A%22radar.itjuzi.com%22%2C%22h%22%3A%22%2F%22%7D; _hp2_id.2147584538=%7B%22userId%22%3A%220126291942326803%22%2C%22pageviewId%22%3A%227482799215015317%22%2C%22sessionId%22%3A%228508113202531986%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%223.0%22%7D; _ga=GA1.2.1131483452.1507775467; _gid=GA1.2.1185526648.1508139248; Hm_lvt_80ec13defd46fe15d2c2dcf90450d14b=1507789511; Hm_lpvt_80ec13defd46fe15d2c2dcf90450d14b=1508142017; gr_session_id_eee5a46c52000d401f969f4535bdaa78=80a8d9a6-f290-48ca-87e9-6b7e23f68d0d; gr_cs1_80a8d9a6-f290-48ca-87e9-6b7e23f68d0d=user_id%3A329559'
}

# random.randint(0, 6)
token = 'b716f2b6a56dd40826b4259519eaa9d08d1992914198301a'
base_url = 'http://192.168.1.201:8000/'
# base_url = 'http://192.168.1.251:8080/'
#接口限速//每隔X秒读取并插入一次（一次15条请求）
insert_rate = 1
#隔X秒更新一次
find_rate = 3600

proxie = {
    'http' : 'http://%s'%iplist[2],
    # 'https': 'https://103.240.10.29:53281'
}


#国内融资
url_invest_in = 'http://radar.itjuzi.com/investevent/info?location=in&orderby=def&page='
#国内并购
url_merge_in = 'http://radar.itjuzi.com/investevent/merg?location=in&orderby=def&page='
#国外融资
url_invest_out = 'http://radar.itjuzi.com/investevent/info?location=out&orderby=def&page='
#国外并购
url_merge_out = 'http://radar.itjuzi.com/investevent/merg?location=out&orderby=def&page='
#公司信息
url_com = 'http://radar.itjuzi.com/company/infonew?page='

def getHtml(url):
    s = requests.Session()
    html = s.get(url, headers=heders, proxies=proxie).content
    return html


def clearfile(path):
    f = open(path, 'r+')
    f.truncate()
    f.close()


def saveInfo(info,path):
    f = open(path, 'a+')
    f.writelines(info)
    f.writelines('\n')
    f.close()


def getInfo(url,path):
    try:
        result = json.loads(getHtml(url))
        status = result['status']
        total = None
        if status == 1:
            total = result['data']['total']
            total = int(total)
            for item in result['data']['rows']:
                thinfo = json.dumps(item)
                saveInfo(thinfo,path)
                # print thinfo
        else:
            print '请求失败'
            print result
        return total
    except Exception as err:
        print err
        return None



# for item in range(12,13,1):
#     url = "http://radar.itjuzi.com/company/infonew?page=%s" % item
#     getInfo(url)
#     print 'page = %d' % item
    # time.sleep(3)


class insetManager():
    def saveMergeInfoToMongo(self,filepath):
        repeat_count = 0
        with open(filepath) as file:
            aaaa = 0
            for line in file:
                dic = json.loads(line)
                dic['investormerge'] = 2
                aaaa = aaaa + 1
                res = requests.post(base_url + 'mongolog/event', data=json.dumps(dic),
                                    headers={'Content-Type': 'application/json','token':token}).content
                res = json.loads(res)
                if res['code'] == 1000:
                    # print '新增merge'
                    pass
                elif res['code'] == 8001:
                    repeat_count = repeat_count + 1
                    print '重复merge'
                    # break
                    # pass
                else:
                    print filepath
                    print '错误数据' + '第%s行' % aaaa
                    print res
                    # break
        return repeat_count


    def saveInvestInfoToMongo(self,filepath):
        repeat_count = 0
        with open(filepath) as file:
            aaaa = 0
            for line in file:
                dic = json.loads(line)
                dic['investormerge'] = 1
                if isinstance(dic['invsest_with'], dict):
                    values = []
                    for key, value in dic['invsest_with'].items():
                        values.append(value)
                    dic['invsest_with'] = values
                aaaa = aaaa + 1
                res = requests.post(base_url + 'mongolog/event', data=json.dumps(dic),
                                    headers={'Content-Type': 'application/json','token':token}).content
                res = json.loads(res)
                if res['code'] == 1000:
                    # print '新增invse'
                    pass
                elif res['code'] == 8001:
                    repeat_count = repeat_count + 1
                    # break
                    # pass
                    print '重复invest'
                else:
                    print filepath
                    print '错误数据' + '第%s行' % aaaa
                    print res
                    break
        return repeat_count


    def saveCompanyInfoToMongo(self, filepath):
        repeat_count = 0
        with open(filepath) as file:
            aaaa = 0
            for line in file:
                aaaa = aaaa + 1
                dic = json.loads(line)
                res = requests.post(base_url + 'mongolog/proj', data=line,
                                    headers={'Content-Type': 'application/json','token':token}).content
                res = json.loads(res)
                if res['code'] == 1000:
                    # print '新增com'
                    pass
                elif res['code'] == 8001:
                    repeat_count = repeat_count + 1
                    # break
                    # pass
                    print '重复company'
                else:
                    print filepath
                    print '错误数据' + '第%s行' % aaaa
                    print res
                    break
        return repeat_count


                    # 获取投资事件
# for item in range(1,102,1):
#     url = "http://radar.itjuzi.com/investevent/merg?location=out&orderby=def&page=%s" % item
#     getInfo(url,'data/itjuzi-data-merg1')
#     url = "http://radar.itjuzi.com/company/infonew?page=%s" % item
#     getInfo(url,'data/itjuzi-data-com1')
#     print 'page = %d' % item
#     time.sleep(3600)



manager = insetManager()

# while True:
#     aa = True
#     page = 1
#     while aa:
#         getInfo(url_merge_in + str(page), 'data2/in-merg1')
#         getInfo(url_com + str(page), 'data2/com1')
#         print 'page = %d' % page
#         page = page + 1
#         aa = False
#     time.sleep(3600)



class invest_in_Thread(threading.Thread):
        def __init__(self, url, path):
            self.url = url
            self.path = path
            threading.Thread.__init__(self)
        def run(self):
            while True:
                aa = True
                page = 1
                repeat_page = 0
                while aa:
                    clearfile(self.path)
                    total = getInfo(self.url + str(page), self.path)
                    repeat_count = manager.saveInvestInfoToMongo(self.path)
                    print '国内投资page = %d' % page
                    page = page + 1
                    if total:
                        max_page = total / 15 +1
                        if page <= max_page:
                            aa = True
                        else:
                            aa = False
                            print '国内投资--结束'
                    time.sleep(insert_rate)
                time.sleep(find_rate)
class invest_out_Thread(threading.Thread):
        def __init__(self, url,path):
            self.url = url
            self.path = path
            threading.Thread.__init__(self)
        def run(self):
            while True:
                aa = True
                page = 1
                while aa:
                    clearfile(self.path)
                    total = getInfo(self.url + str(page), self.path)
                    manager.saveInvestInfoToMongo(self.path)
                    print '国外投资page = %d' % page
                    page = page + 1
                    if total:
                        max_page = total / 15 +1
                        if page <= max_page:
                            aa = True
                        else:
                            aa = False
                            print '国外投资--结束'
                    time.sleep(insert_rate)
                time.sleep(find_rate)
class merge_in_Thread(threading.Thread):
        def __init__(self, url,path):
            self.url = url
            self.path = path
            threading.Thread.__init__(self)
        def run(self):
            while True:
                aa = True
                page = 1
                while aa:
                    clearfile(self.path)
                    total = getInfo(self.url + str(page), self.path)
                    manager.saveMergeInfoToMongo(self.path)
                    print '国内并购page = %d' % page
                    page = page + 1
                    if total:
                        max_page = total / 15 +1
                        if page <= max_page:
                            aa = True
                        else:
                            aa = False
                            print '国内并购--结束'
                    time.sleep(insert_rate)
                time.sleep(find_rate)
class merge_out_Thread(threading.Thread):
        def __init__(self, url,path):
            self.url = url
            self.path = path
            threading.Thread.__init__(self)
        def run(self):
            while True:
                aa = True
                page = 1
                while aa:
                    clearfile(self.path)
                    total = getInfo(self.url + str(page), self.path)
                    manager.saveMergeInfoToMongo(self.path)
                    print '国外并购page = %d' % page
                    page = page + 1
                    if total:
                        max_page = total / 15 +1
                        if page <= max_page:
                            aa = True
                        else:
                            aa = False
                            print '国外并购--结束'
                    time.sleep(insert_rate)
                time.sleep(find_rate)
class company_in_Thread(threading.Thread):
        def __init__(self, url,path):
            self.url = url
            self.path = path
            threading.Thread.__init__(self)
        def run(self):
            while True:
                aa = True
                page = 4366
                while aa:
                    clearfile(self.path)
                    url = self.url + str(page)
                    total = getInfo(url, self.path)
                    manager.saveCompanyInfoToMongo(self.path)
                    print '公司page = %d' % page
                    page = page + 1
                    if total:
                        max_page = total / 15 +1
                        if page <= max_page:
                            aa = True
                        else:
                            aa = False
                            print '公司--结束'
                    time.sleep(insert_rate)
                time.sleep(find_rate)


invest_in_Thread(url_invest_in,'data3/invest_in').start()
invest_out_Thread(url_invest_out,'data3/invest_out').start()
merge_in_Thread(url_merge_in, 'data3/merge_in').start()
merge_out_Thread(url_merge_out,'data3/merge_out').start()
company_in_Thread(url_com,'data3/company_in').start()

